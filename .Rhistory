setwd("C:/Users/26som/Desktop/Drexel/Subjects_2020_spring/Data Mining/Final_Project/final_code")
## Load Libraries
library(e1071)
library(caret)
library(corrplot)
library(mice)
library(DMwR)
library(rpart)
library(rpart.plot)
library(randomForest)
library(nnet)
library(e1071)
library(factoextra)
library(cluster)
library(fpc)
library(gridExtra)
## Load Data
bank <- read.csv('bank-full.csv')
## Load Data
bank <- read.csv('bank-full.csv')
setwd("C:/Users/26som/Desktop/Drexel/Subjects_2020_spring/Data Mining/Final_Project/final_code")
## Load Data
bank <- read.csv('bank-full.csv')
setwd("C:/Users/26som/Desktop/Drexel/Subjects_2020_spring/Data Mining/Final_Project/final_code")
## Load Libraries
library(e1071)
library(caret)
library(corrplot)
library(mice)
library(DMwR)
library(rpart)
library(rpart.plot)
library(randomForest)
library(nnet)
library(e1071)
library(factoextra)
library(cluster)
library(fpc)
library(gridExtra)
## Load Data
bank <- read.csv('bank-full.csv')
## Load Data
bank <- read.csv('bank_full.csv')
## View the structure and summary information for the bank data.
str(bank)
summary(bank)
# variable poutcome contains more than 75 % of the records which
# are unknown. Value 'unknown' are the null records in the data.
table(bank$poutcome)
bank <- bank[ , !(names(bank) %in% "poutcome")]
#previous
table(bank$previous, bank$y)
# 'previous' variable has value ranging from 0-275 which contains value '0' for
# more than 75% of the records i.e. no contacts performed before this campaign for
#most of the records Data is skewed. This colummn contains outliers and is not significant.
bank <- bank[ , !(names(bank) %in% "previous")]
# Histogram
hist(bank$pdays,
main="pdays",
xlab="",
col="steelblue")
# Density plot
plot(density(bank$pdays), main="pdays")
bank <- bank[ , !(names(bank) %in% "pdays")]
#Correltation
nums <- unlist(lapply(bank, is.numeric))
cor(bank[, nums])
# to identify any redundant variables
corrplot(cor(bank[ , nums]), method="circle")
# We can perform a Chi-Square Test for Independence between the two categorical
# variables
# H0: The two variables are not dependent
# H1: The two variables are dependent
table(bank$job, bank$y)
chisq.test(table(bank$job, bank$y))
table(bank$marital, bank$y)
chisq.test(table(bank$marital, bank$y))
table(bank$education, bank$y)
chisq.test(table(bank$education, bank$y))
table(bank$contact, bank$y)
chisq.test(table(bank$contact, bank$y))
# Identifying Outliers
# Boxplot
# Using a boxplot, we can visually identify outliers as those points
# that extend beyond the whiskers and use $out
boxplot(bank$campaign, main="campaign")
#outlier <- boxplot.stats(bank$campaign)$out
boxplot(bank$duration, main="duration")
boxplot(bank$balance, main="balance")
boxplot(bank$age, main="age")
# absolute value of the Z-Score is greater than 3
nrow(bank[abs(scale(bank$campaign))>3,])
nrow(bank[abs(scale(bank$duration))>3,])
nrow(bank[abs(scale(bank$balance))>3,])
nrow(bank[abs(scale(bank$age))>3,])
# For variable 'campaign', large number (>12) of contacts performed seems
# unusual values in this dataset, and they can distort statistical analyses
# Since there are very less number of outliers, Dropping it.
#bank <- bank[!bank$campaign %in% outlier,]
bank <- bank[abs(scale(bank$campaign))<=3,]
# Variables 'duration', 'balance', 'age' are actual values which
# can not be neglected. Capping the data.
# replace the outliers which are less than (Q1 - 1.5*IQR) with the 5th percentile and
# replace the outliers which are greater than (Q3 + 1.5*IQR) with the 95th percentile of the data.
# Ref: https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba
# Ref: https://stackoverflow.com/questions/13339685/how-to-replace-outliers-with-the-5th-and-95th-percentile-values-in-r
hist(bank$duration,
main="duration",
xlab="",
col="steelblue")
hist(bank$balance,
main="balance",
xlab="",
col="steelblue")
hist(bank$age,
main="age",
xlab="",
col="steelblue")
fun <- function(x){
quantiles <- quantile( x, c(0.05, 0.25, 0.75, 0.95 ) )
IQR = quantiles[3] - quantiles[2]
x[ x < quantiles[1] ] <- (quantiles[2] - 1.5*IQR)
x[ x > quantiles[4] ] <- (quantiles[3] + 1.5*IQR)
x
}
bank$duration <- fun(bank$duration)
bank$balance <- fun(bank$balance)
bank$age <- fun(bank$age)
# summary
summary(bank)
# After looking at the summary, variables 'job', 'education', 'contact'
# contains value 'unknown' which are the  missing records in the data.
# Replacing such records as NULL
bank[bank == "unknown"] <- NA
bank$job <- factor(bank$job)
bank$education <- factor(bank$education)
bank$contact <- factor(bank$contact)
summary(bank)
# Determine how many rows are missing values:
nrow(bank[!complete.cases(bank),])
# To identify duplicate observations
bank[duplicated(bank),]
# Imputing missing values using mice package
# Since all the 3 variables are categorical, using method = "polyreg".
# Using Polytomous logistic regression to predict the level of missing data.
# Number of multiple imputations: m = 5
# Number of iterations: maxit = 5
# Ref: https://datascienceplus.com/handling-missing-data-with-mice-package-a-simple-approach/
init = mice(bank, maxit=0)
meth = init$method
predM = init$predictorMatrix
meth[c("job")]="polyreg"
meth[c("education")]="polyreg"
meth[c("contact")]="polyreg"
set.seed(123)
bank_imputed = mice(bank, method=meth, predictorMatrix=predM, m=5)
bank_imputed_df <- complete(bank_imputed)
summary(bank_imputed_df)
nrow(bank[!complete.cases(bank_imputed_df),])
# Label encoding on month variable.
bank2 <- bank_imputed_df
bank2$month <- match(tolower(bank2$month),tolower(month.abb))
summary(bank2)
# Creating dummy variables, conveting categorial variables.
dum <- dummyVars(~job+marital+education+default+housing+loan+contact, data=bank2,
sep="_", fullRank = TRUE)
df <- predict(dum, bank2)
bank3 <- data.frame(bank2[,!names(bank2) %in% c("job", "marital", "education", "default", "housing", "loan", "contact")], df)
summary(bank3)
# Data for kmeans
bank_kmeans <- bank3
bank_kmeans_y <- bank_kmeans[,7]
bank_kmeans=bank_kmeans[,-7]
# Using Scaled Data for kmeans
# Scaling only first 6 variables since they are continuous
# or ordinal variable while others are binary variables.
bank_kmeans_scaled <- sapply(bank_kmeans[,1:6], FUN = scale)
bank_kmeans <- bank_kmeans[,7:24]
bank_kmeans <- cbind(bank_kmeans_scaled, bank_kmeans)
# Choose k  (plot total sum of squares)
tss<-rep(1,9)
kmeans1<-rep(1,9)
for (k in 1:9) {
set.seed(123)
kmeans1[k]=list(kmeans(bank_kmeans, centers=k, trace=FALSE, nstart=30))
tss[k]=kmeans1[[k]]$tot.withinss
}
plot(1:9,tss)
# From above plot, its not proper elbow, so
# lets view for cluster 2, 3, 4, 5
grid.arrange(fviz_cluster(kmeans1[[2]], bank_kmeans),
fviz_cluster(kmeans1[[3]], bank_kmeans),
fviz_cluster(kmeans1[[4]], bank_kmeans),
fviz_cluster(kmeans1[[5]], bank_kmeans),
nrow = 2)
dev.off()
# Since our data contains more binary variables,
# instead of using euclidean distance, we are using
# manhattan distance. Since our data is large
# we are using clara function which considers a small
# sample of the data with fixed size (sampsize) and applies
# the PAM algorithm to generate an optimal set of medoids for the sample.
cl2 <- clara(bank_kmeans, 2, metric = "manhattan",
samples = 10000, pamLike = TRUE)
cl2$clusinfo
fviz_cluster(cl2, bank_kmeans)
cl3 <- clara(bank_kmeans, 3, metric = "manhattan",
samples = 10000, pamLike = TRUE)
cl3$clusinfo
fviz_cluster(cl3, bank_kmeans)
cl4 <- clara(bank_kmeans, 4, metric = "manhattan",
samples = 10000, pamLike = TRUE)
cl4$clusinfo
fviz_cluster(cl4, bank_kmeans)
cl5 <- clara(bank_kmeans, 5, metric = "manhattan",
samples = 10000, pamLike = TRUE)
cl5$clusinfo
fviz_cluster(cl5, bank_kmeans)
# The mean of the dissimilarities of the observations
# to their closest medoid. This is used as a measure of
# the goodness of the clustering.
cl3$clusinfo
## External Validation
table(y=bank_kmeans_y, Cluster=cl3$clustering)
# Using Min-Max Normalization for classification
prepObj <- preProcess(x=bank3, method="range")
bank3 <- predict(prepObj, bank3)
## Training & Testing
# Splitting the data into training and
# testing sets using a 80/20 split rule
set.seed(123)
samp <- createDataPartition(bank3$y, p=.80, list=FALSE)
train = bank3[samp, ]
test = bank3[-samp, ]
# We can view the distribution of the dependent variable before resampling
table(train$y)
## SMOTE Synthetic Minority Oversampling Technique
set.seed(123)
train_sm <- SMOTE(y~., data=train)
# We can look at the class distribution after
table(train_sm$y)
# Feature selection
## Decision Trees
can.rpart <- rpart(formula = y ~ .,
data = train_sm,
method = "class")
# We can see the basic output of our tree
can.rpart
rpart.plot(can.rpart)
# We can view the variable importance, which is an element of
# our decision tree object, can.rpart
can.rpart$variable.importance
## Random Forest
set.seed(123)
can.rf <- randomForest(y~.,
data=train_sm,
importance=TRUE,
ntree=500)
# We can view the output from our model
can.rf
# We can view the importance information for each class
can.rf$importance
# We can view the variable importance plot
varImpPlot(can.rf)
# Decision Tree Model
varImp(can.rpart)
# Random Forest Model
varImp(can.rf)
set.seed(123)
control <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
number = 10,
repeats = 3,
verbose = FALSE)
can_rfe <- rfe(x = train_sm[,-7],
y = train_sm$y,
rfeControl = control)
